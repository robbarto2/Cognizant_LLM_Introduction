{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69ac735",
   "metadata": {},
   "source": [
    "# Lesson 3 Demo Notebook — Training Strategies & Transfer\n",
    "\n",
    "This notebook mirrors the lesson flow with small, CPU-friendly demos. Each section is short and practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02017fe3",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "Install optional libraries if needed. All demos are tiny; they run on CPU. If you don't want to install anything, you can still run the pure-Python parts (Sections 3, 5, 6, 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5df6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip -q install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip -q install transformers==4.42.4 peft==0.11.1 accelerate==0.32.1 datasets==2.20.0 scikit-learn==1.4.2\n",
    "# If you need a lightweight FAISS alternative for RAG demo, we'll do cosine over TF-IDF via scikit-learn.\n",
    "import math, random, time, os, json\n",
    "from collections import defaultdict, Counter\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c62838",
   "metadata": {},
   "source": [
    "## 1) Strategy ladder: start with prompting → add RAG if facts matter\n",
    "We show a **no-training** baseline and a tiny RAG sketch with TF‑IDF retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95328d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How can I adapt a base model cheaply?\n",
      "Top docs:\n",
      "  1. RAG systems retrieve documents at query time so the model can ground its answer.\n",
      "  2. LoRA adapters let you fine-tune a small number of weights to steer a base model.\n",
      "------------------------------------------------------------\n",
      "Q: How do I keep answers up-to-date with sources?\n",
      "Top docs:\n",
      "  1. LoRA adapters let you fine-tune a small number of weights to steer a base model.\n",
      "  2. Teacher forcing feeds the correct next token at train time for stability.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tiny 'corpus' and a trivial retrieval function using TF-IDF (scikit-learn) if available, else fallback.\n",
    "docs = [\n",
    "    \"LoRA adapters let you fine-tune a small number of weights to steer a base model.\",\n",
    "    \"RAG systems retrieve documents at query time so the model can ground its answer.\",\n",
    "    \"Gradient accumulation simulates larger batches by summing gradients across steps.\",\n",
    "    \"Early stopping halts training when the validation metric stops improving.\",\n",
    "    \"Teacher forcing feeds the correct next token at train time for stability.\",\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    \"How can I adapt a base model cheaply?\",\n",
    "    \"How do I keep answers up-to-date with sources?\",\n",
    "]\n",
    "\n",
    "def simple_retriever(q, docs, k=2):\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        tfidf = TfidfVectorizer().fit(docs+[q])\n",
    "        m = tfidf.transform(docs)\n",
    "        v = tfidf.transform([q])\n",
    "        sims = cosine_similarity(v, m)[0]\n",
    "        idx = sims.argsort()[-k:][::-1]\n",
    "        return [docs[i] for i in idx], [float(sims[i]) for i in idx]\n",
    "    except Exception as e:\n",
    "        # Fallback: keyword overlap\n",
    "        qset = set(q.lower().split())\n",
    "        scored = [(sum(1 for w in d.lower().split() if w in qset), d) for d in docs]\n",
    "        scored.sort(reverse=True)\n",
    "        return [d for _, d in scored[:k]], None\n",
    "\n",
    "for q in queries:\n",
    "    ctx, score = simple_retriever(q, docs, k=2)\n",
    "    print(f\"Q: {q}\\nTop docs:\")\n",
    "    for i,c in enumerate(ctx,1):\n",
    "        print(f\"  {i}. {c}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df671a9",
   "metadata": {},
   "source": [
    "## 2) Teacher forcing: clean pairs & masking\n",
    "We simulate SFT with prompt→target pairs and compute loss **only** on target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f46d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids:    [1, 3, 4, 9, 4, 6, 7, 9]\n",
      "Target ids:   [3, 4, 9, 4, 6, 7, 9, 2]\n",
      "Loss mask:    [0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Decoded demo: ['<bos>', 'classify', 'cats', '.', 'cats', 'are', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "# Toy vocabulary & data\n",
    "PAD, BOS, EOS = \"<pad>\", \"<bos>\", \"<eos>\"\n",
    "vocab = [PAD,BOS,EOS,\"classify\",\"cats\",\"dogs\",\"are\",\"great\",\"ok\",\".\"]\n",
    "tok2id = {t:i for i,t in enumerate(vocab)}\n",
    "id2tok = {i:t for t,i in tok2id.items()}\n",
    "\n",
    "def tokenize(seq):\n",
    "    return [tok2id.get(t, tok2id[PAD]) for t in seq]\n",
    "\n",
    "# Two clean pairs: (prompt)->(target)\n",
    "pairs = [\n",
    "    ([BOS, \"classify\", \"cats\", \".\"], [\"cats\",\"are\",\"great\",\".\",EOS]),\n",
    "    ([BOS, \"classify\", \"dogs\", \".\"], [\"dogs\",\"are\",\"ok\",\".\",EOS]),\n",
    "]\n",
    "\n",
    "# Build a single training batch of token ids with masks: we predict only on targets.\n",
    "def build_batch(pairs):\n",
    "    batch_in, batch_tgt, loss_mask = [], [], []\n",
    "    for prompt, target in pairs:\n",
    "        x = tokenize(prompt + target)     # concat for causal setup\n",
    "        # loss on positions that belong to 'target' portion only\n",
    "        mask = [0]*len(prompt) + [1]*len(target)\n",
    "        batch_in.append(x[:-1])\n",
    "        batch_tgt.append(x[1:])\n",
    "        loss_mask.append(mask[1:])  # align with next-token targets\n",
    "    return batch_in, batch_tgt, loss_mask\n",
    "\n",
    "X, Y, M = build_batch(pairs)\n",
    "print(\"Input ids:   \", X[0])\n",
    "print(\"Target ids:  \", Y[0])\n",
    "print(\"Loss mask:   \", M[0])\n",
    "print(\"Decoded demo:\", [id2tok[i] for i in X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477bfc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: masked loss over target-only tokens = 2.242\n",
      "Example 1: masked loss over target-only tokens = 2.522\n"
     ]
    }
   ],
   "source": [
    "# Compute a toy cross-entropy loss where mask==1; random logits to illustrate masking effect\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "def masked_ce(logits, target_ids, mask):\n",
    "    # logits: [T, V], target_ids: [T], mask: [T] (0/1)\n",
    "    T = len(target_ids)\n",
    "    probs = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "    nll = -np.log(probs[np.arange(T), target_ids] + 1e-12)\n",
    "    masked = nll * np.array(mask)\n",
    "    denom = max(1, sum(mask))\n",
    "    return masked.sum()/denom\n",
    "\n",
    "for i in range(len(X)):\n",
    "    T = len(X[i])\n",
    "    logits = np.random.randn(T, vocab_size) * 0.5  # dummy model\n",
    "    loss = masked_ce(logits, Y[i], M[i])\n",
    "    print(f\"Example {i}: masked loss over target-only tokens = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35cbbf",
   "metadata": {},
   "source": [
    "## 3) Transfer learning options (sketch)\n",
    "Below are **minimal training stubs** showing how you would:\n",
    "- attach **LoRA adapters** (lightweight)\n",
    "- do **last-layer** tuning\n",
    "- or **full fine-tuning**\n",
    "\n",
    "> These are illustrative; set `steps=10` for a CPU demo. Uncomment pip installs above first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316028be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE / MINIMAL: requires transformers + peft installed to actually run\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# model_name = \"sshleifer/tiny-gpt2\"  # tiny model for demo\n",
    "# tok = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# --- Adapters (LoRA) ---\n",
    "# peft_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, target_modules=['c_attn'])\n",
    "# model_lora = get_peft_model(model, peft_cfg)\n",
    "\n",
    "# --- Last-layer FT --- (freeze all but final lm_head)\n",
    "# for n,p in model.named_parameters():\n",
    "#     p.requires_grad = ('lm_head' in n)\n",
    "\n",
    "# --- Full FT --- (no freezing)\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "\n",
    "# --- Train with small steps ---\n",
    "# args = TrainingArguments(output_dir=\"out\", per_device_train_batch_size=2, num_train_epochs=1,\n",
    "#                          learning_rate=5e-4, logging_steps=1, max_steps=10)\n",
    "# trainer = Trainer(model=model_lora, args=args, train_dataset=your_dataset)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52875af6",
   "metadata": {},
   "source": [
    "## 4) Batch-size scaling without a big GPU\n",
    "We compute **effective batch size** (EBS) and show gradient accumulation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f43eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective batch size: 8 × 4 × 2 = 64\n",
      "Pseudo-loop:\n",
      "Optimizer step 0:\n",
      "  micro-batch 1/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 2/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 3/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 4/4: forward -> backward (grads accumulate)\n",
      "  clip gradients -> optimizer.step() -> zero_grad()\n",
      "Optimizer step 1:\n",
      "  micro-batch 1/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 2/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 3/4: forward -> backward (grads accumulate)\n",
      "  micro-batch 4/4: forward -> backward (grads accumulate)\n",
      "  clip gradients -> optimizer.step() -> zero_grad()\n"
     ]
    }
   ],
   "source": [
    "def effective_batch_size(micro_batch, accum_steps, replicas):\n",
    "    return micro_batch * accum_steps * replicas\n",
    "\n",
    "def demo_grad_accum(micro_batch=8, accum_steps=4, replicas=2):\n",
    "    ebs = effective_batch_size(micro_batch, accum_steps, replicas)\n",
    "    print(f\"Effective batch size: {micro_batch} × {accum_steps} × {replicas} = {ebs}\")\n",
    "    print(\"Pseudo-loop:\")\n",
    "    for step in range(2):\n",
    "        print(f\"Optimizer step {step}:\")\n",
    "        for a in range(accum_steps):\n",
    "            print(f\"  micro-batch {a+1}/{accum_steps}: forward -> backward (grads accumulate)\")\n",
    "        print(\"  clip gradients -> optimizer.step() -> zero_grad()\")\n",
    "\n",
    "demo_grad_accum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf938bd1",
   "metadata": {},
   "source": [
    "## 5) Loss spike triage (simulation)\n",
    "We simulate a noisy loss curve, then apply quick fixes: lower LR and gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b02361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fixes: mean=0.651, spike at step 30 -> 1.000\n",
      "After fixes:  mean=0.665, step 30 -> 0.555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "steps = 60\n",
    "loss = 2.0*np.exp(-np.linspace(0,3,steps)) + 0.05*np.random.randn(steps)\n",
    "# Inject a 'spike'\n",
    "loss[30] += 0.6\n",
    "\n",
    "def smooth(x, k=3):\n",
    "    y = np.copy(x)\n",
    "    for i in range(k, len(x)):\n",
    "        y[i] = 0.6*y[i] + 0.4*y[i-1]\n",
    "    return y\n",
    "\n",
    "print(\"Before fixes: mean={:.3f}, spike at step 30 -> {:.3f}\".format(loss.mean(), loss[30]))\n",
    "loss_fixed = smooth(loss)\n",
    "loss_fixed[30] *= 0.7  # pretend lowering LR + clipping helped\n",
    "print(\"After fixes:  mean={:.3f}, step 30 -> {:.3f}\".format(loss_fixed.mean(), loss_fixed[30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539ce9e",
   "metadata": {},
   "source": [
    "## 6) Decision checklist helper\n",
    "A tiny function that recommends the **lowest rung** given task, data, and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ae0b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'classify', 'data_size': 'small', 'labels': True, 'facts_change': False, 'latency_strict': True} -> Adapters → Last-layer if needed\n",
      "{'task': 'qa', 'data_size': 'small', 'labels': False, 'facts_change': True, 'latency_strict': False} -> Prompt + RAG (then small adapters if formatting brittle)\n",
      "{'task': 'write', 'data_size': 'medium', 'labels': False, 'facts_change': False, 'latency_strict': False} -> Adapters → Full FT if style still off\n"
     ]
    }
   ],
   "source": [
    "def choose_strategy(task, data_size, labels, facts_change, latency_strict):\n",
    "    # Very rough, didactic mapping\n",
    "    if facts_change:  # knowledge shifts -> prefer RAG\n",
    "        return \"Prompt + RAG (then small adapters if formatting brittle)\"\n",
    "    if labels and task in {\"classify\",\"extract\"}:\n",
    "        if data_size in {\"small\",\"medium\"}:\n",
    "            return \"Adapters → Last-layer if needed\"\n",
    "        else:\n",
    "            return \"Last-layer FT\"\n",
    "    if task in {\"write\",\"generate\"}:\n",
    "        if data_size == \"small\":\n",
    "            return \"Adapters\"\n",
    "        else:\n",
    "            return \"Adapters → Full FT if style still off\"\n",
    "    # default\n",
    "    return \"Prompt first; escalate as needed\"\n",
    "\n",
    "examples = [\n",
    "    dict(task=\"classify\", data_size=\"small\", labels=True, facts_change=False, latency_strict=True),\n",
    "    dict(task=\"qa\", data_size=\"small\", labels=False, facts_change=True, latency_strict=False),\n",
    "    dict(task=\"write\", data_size=\"medium\", labels=False, facts_change=False, latency_strict=False),\n",
    "]\n",
    "for ex in examples:\n",
    "    print(ex, \"->\", choose_strategy(**ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafadb6",
   "metadata": {},
   "source": [
    "## 7) Measurement: tokens/sec, early stopping, run log\n",
    "We show a simple timer-based tokens/sec, a mock early-stopping loop, and a CSV run log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2edd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens/sec (mock): 98830\n",
      "Step 1: new best 0.700\n",
      "Step 2: new best 0.750\n",
      "Step 3: new best 0.760\n",
      "Step 4: no improvement (bad=1)\n",
      "Step 5: no improvement (bad=2)\n",
      "Step 6: no improvement (bad=3)\n",
      "Early stop at step 6. Best=0.760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/run_log.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, csv, tempfile, os, math, random\n",
    "\n",
    "def tokens_per_second(tokens, seconds):\n",
    "    return tokens / max(1e-6, seconds)\n",
    "\n",
    "# Demo timer\n",
    "start = time.time(); tokens = 5000\n",
    "time.sleep(0.05)  # pretend to process\n",
    "tps = tokens_per_second(tokens, time.time()-start)\n",
    "print(f\"Tokens/sec (mock): {tps:.0f}\")\n",
    "\n",
    "# Early stopping mock: stop if no improvement after 'patience' checks by at least 'min_delta'\n",
    "def early_stop_demo(metric_values, patience=3, min_delta=0.0):\n",
    "    best = -1e9; bad = 0\n",
    "    for i, m in enumerate(metric_values, 1):\n",
    "        if m > best + min_delta:\n",
    "            best, bad = m, 0\n",
    "            print(f\"Step {i}: new best {best:.3f}\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            print(f\"Step {i}: no improvement (bad={bad})\")\n",
    "        if bad >= patience:\n",
    "            print(f\"Early stop at step {i}. Best={best:.3f}\")\n",
    "            break\n",
    "\n",
    "metric_vals = [0.70,0.75,0.76,0.761,0.761,0.761,0.762]\n",
    "early_stop_demo(metric_vals, patience=3, min_delta=0.001)\n",
    "\n",
    "# Minimal run log\n",
    "runlog_path = \"data/run_log.csv\"\n",
    "with open(runlog_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"date\",\"config_hash\",\"EBS\",\"LR\",\"best_metric\",\"next_action\"])\n",
    "    w.writerow([\"2025-09-19\",\"cfg_7b_lora_v2\",\"64\",\"2e-4\",\"EM 88.1\",\"Hold; try LR 1.5e-4\"])\n",
    "    w.writerow([\"2025-09-19\",\"cfg_7b_llft_v1\",\"96\",\"3e-4\",\"Acc 90.2\",\"Stop; met target\"])\n",
    "runlog_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d51714",
   "metadata": {},
   "source": [
    "## 8) Summary\n",
    "- Start at the lowest rung; escalate only when the metric demands.\n",
    "- Keep pairs clean and masks correct.\n",
    "- Scale effective batch via micro-batch × accumulation × replicas; LR ≈ scales with EBS.\n",
    "- Use the triage playbook for loss spikes; measure honestly with dev set, tokens/sec, and early stop.\n",
    "\n",
    "**Next:** swap in your real dataset and base model, and re-run the same scaffolding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
