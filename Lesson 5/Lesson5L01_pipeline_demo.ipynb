{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936906d3",
   "metadata": {},
   "source": [
    "\n",
    "# Module 3 — Lesson 1: Data I/O for Training (Hands-on)\n",
    "This notebook illustrates the **practical pipeline** you built in Lesson 1:\n",
    "- A tiny **synthetic dataset** with `PAD/BOS/EOS` and a controllable **unknown-token rate**.\n",
    "- **Length-aware batching** (bins) and a **collate_fn** that pads per-batch and builds `attention_mask`.\n",
    "- A **DataLoader** configured with `num_workers`, `prefetch_factor`, and `persistent_workers`.\n",
    "- **Tokens/sec** logging and a **micro overfit test** on a tiny batch.\n",
    "- A peek at **DistributedSampler** (works in single-process for demo).\n",
    "\n",
    "> You can run this on CPU if you don't have a GPU; it just runs slower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137c3598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# !pip install torch --quiet  # Uncomment if running in a fresh env\n",
    "import math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89562671",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Tiny synthetic tokenizer + dataset\n",
    "We emulate a tokenizer with a small vocabulary and insert:\n",
    "- **BOS=1**, **EOS=2**, **PAD=0**, **UNK=3**\n",
    "- A controllable **unknown-token rate** to show how to measure it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b50ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 152)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
    "VOCAB_SIZE = 200   # toy vocabulary (ids: 0..199)\n",
    "\n",
    "def fake_tokenize(text: str, unk_prob: float = 0.02, max_word_id: int = 199) -> List[int]:\n",
    "    \"\"\"Turn characters into token IDs for demo; inject UNKs with probability unk_prob.\"\"\"\n",
    "    ids = [BOS]\n",
    "    for ch in text:\n",
    "        if ch == \" \":\n",
    "            continue\n",
    "        if random.random() < unk_prob:\n",
    "            tok = UNK\n",
    "        else:\n",
    "            tok = 4 + (ord(ch) % (max_word_id-4))  # map to [4..max_word_id]\n",
    "        ids.append(tok)\n",
    "    ids.append(EOS)\n",
    "    return ids\n",
    "\n",
    "def make_corpus(n=5000, min_len=20, max_len=200, unk_prob=0.02):\n",
    "    random.seed(0)\n",
    "    corpus = []\n",
    "    for _ in range(n):\n",
    "        L = random.randint(min_len, max_len)\n",
    "        text = \"\".join(random.choice(\"abcdefghijklmnopqrstuvwxyz \") for _ in range(L))\n",
    "        ids = fake_tokenize(text, unk_prob=unk_prob)\n",
    "        corpus.append(ids)\n",
    "    return corpus\n",
    "\n",
    "corpus = make_corpus(n=6000, min_len=10, max_len=300, unk_prob=0.02)\n",
    "len(corpus), sum(len(x) for x in corpus)//len(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b10bbd",
   "metadata": {},
   "source": [
    "\n",
    "### Measure unknown-token rate & a quick length histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04aa30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9552834696917412, {'<=128': 2437, '<=512': 3563, '<=1024': 0, '>1024': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def unknown_token_rate(seqs: List[List[int]]) -> float:\n",
    "    total = sum(len(s) for s in seqs)\n",
    "    unk = sum(s.count(UNK) for s in seqs)\n",
    "    return 100.0 * unk / max(1,total)\n",
    "\n",
    "def length_histogram(seqs: List[List[int]], cuts=(128,512,1024)):\n",
    "    buckets = {\"<=128\":0, \"<=512\":0, \"<=1024\":0, \">1024\":0}\n",
    "    for s in seqs:\n",
    "        L = len(s)\n",
    "        if L <= 128: buckets[\"<=128\"] += 1\n",
    "        elif L <= 512: buckets[\"<=512\"] += 1\n",
    "        elif L <= 1024: buckets[\"<=1024\"] += 1\n",
    "        else: buckets[\">1024\"] += 1\n",
    "    return buckets\n",
    "\n",
    "unk_rate = unknown_token_rate(corpus)\n",
    "hist = length_histogram(corpus)\n",
    "unk_rate, hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149fb96",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Dataset + length-aware batching\n",
    "We create a `Dataset` and a small **bucketing** step (3 bins). We then build **batches** from each bin and shuffle the order of batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6af4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2437, 3563, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, seqs: List[List[int]]):\n",
    "        self.seqs = seqs\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.seqs[idx]\n",
    "        return {\"input_ids\": torch.tensor(s, dtype=torch.long)}\n",
    "\n",
    "dataset = ToyDataset(corpus)\n",
    "\n",
    "# Build 3 length bins and pre-make batches (indices)\n",
    "def build_length_bins(lengths, cuts=(128,512)):\n",
    "    short, med, long = [], [], []\n",
    "    for i, L in enumerate(lengths):\n",
    "        if L <= cuts[0]: short.append(i)\n",
    "        elif L <= cuts[1]: med.append(i)\n",
    "        else: long.append(i)\n",
    "    return short, med, long\n",
    "\n",
    "lengths = [len(x) for x in corpus]\n",
    "short, med, long_ = build_length_bins(lengths, cuts=(128,512))\n",
    "len(short), len(med), len(long_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e2b866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376, [3628, 1461, 5918, 4667, 5023])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_batches(idxs: List[int], batch_size: int) -> List[List[int]]:\n",
    "    random.shuffle(idxs)\n",
    "    return [idxs[i:i+batch_size] for i in range(0, len(idxs), batch_size)]\n",
    "\n",
    "BS = 16  # per-iterator batch size (per-GPU if distributed)\n",
    "batches = make_batches(short, BS) + make_batches(med, BS) + make_batches(long_, BS)\n",
    "random.shuffle(batches)\n",
    "len(batches), batches[0][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd1669",
   "metadata": {},
   "source": [
    "\n",
    "### Collate with per-batch padding + attention mask\n",
    "We pad each batch to its **own** max length and build `attention_mask` so the model ignores padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4b3c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 282]),\n",
       " torch.Size([16, 282]),\n",
       " torch.Size([16, 282]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_pad(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    seqs = [item[\"input_ids\"] for item in batch]\n",
    "    max_len = max(s.size(0) for s in seqs)\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=PAD)\n",
    "    attn = (padded != PAD).long()\n",
    "    # Labels = next-token with shift by 1; PAD on last position\n",
    "    labels = padded.clone()\n",
    "    labels[:, :-1] = padded[:, 1:]\n",
    "    labels[:, -1] = PAD\n",
    "    return {\"input_ids\": padded, \"attention_mask\": attn, \"labels\": labels}\n",
    "\n",
    "# Quick smoke test\n",
    "bt = [dataset[i] for i in batches[0]]\n",
    "sample = collate_pad(bt)\n",
    "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape, sample[\"labels\"].shape, sample[\"attention_mask\"][0,:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42697d",
   "metadata": {},
   "source": [
    "\n",
    "## 3) BatchSampler + DataLoader (workers, prefetch, persistence)\n",
    "We use a **BatchSampler** so each yielded item is already a batch of indices.  \n",
    "Tune `num_workers` and `prefetch_factor` gradually (e.g., 2→4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00080ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 282])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class PrebuiltBatchSampler(Sampler[List[int]]):\n",
    "    def __init__(self, batches: List[List[int]]):\n",
    "        self._batches = batches\n",
    "    def __iter__(self):\n",
    "        return iter(self._batches)\n",
    "    def __len__(self):\n",
    "        return len(self._batches)\n",
    "\n",
    "# Notebook-safe loader settings\n",
    "NUM_WORKERS = 0          # <-- key change (was 2)\n",
    "PREFETCH    = None       # ignored when workers=0\n",
    "PERSISTENT  = False      # must be False when workers=0\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=PrebuiltBatchSampler(batches),\n",
    "    collate_fn=collate_pad,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "\n",
    "next(iter(loader))[\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45ba5c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Tiny language model head + masked loss\n",
    "A minimal model: embeddings + linear output; compute cross-entropy **only on non-PAD** positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02720958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_12951/886941677.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        x = self.embed(input_ids)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Flatten\n",
    "            logits_f = logits.view(-1, logits.size(-1))\n",
    "            labels_f = labels.view(-1)\n",
    "            mask_f = attention_mask.view(-1).float()\n",
    "            # Cross-entropy per token\n",
    "            ce = nn.functional.cross_entropy(logits_f, labels_f, reduction=\"none\")\n",
    "            # Mask out PAD positions\n",
    "            loss = (ce * mask_f).sum() / mask_f.sum().clamp_min(1.0)\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "model = TinyLM().to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9f03e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Tokens/sec logging + micro overfit test\n",
    "- We measure **tokens/sec** by summing `attention_mask` (real tokens only).\n",
    "- **Micro overfit test**: overfit on 1 tiny batch to ensure labels/masks are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10731cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/sswmwmgd3bl8rxfdhchzt6h00000gn/T/ipykernel_12951/7848881.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens_sec': 651858.2996219884, 'steps': 30, 'seconds': 0.10469913482666016}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def run_steps(dataloader, steps=50, accum_steps=1):\n",
    "    model.train()\n",
    "    tokens_seen = 0\n",
    "    t0 = time.time()\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step >= steps: break\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "        tokens_seen += int(attention_mask.sum().item())\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = out[\"loss\"] / accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step + 1) % accum_steps == 0:\n",
    "            scaler.step(optim); scaler.update()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    tps = tokens_seen / max(1e-9, dt)\n",
    "    return {\"tokens_sec\": tps, \"steps\": steps, \"seconds\": dt}\n",
    "\n",
    "# Quick 30-step run\n",
    "metrics = run_steps(loader, steps=30, accum_steps=2)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bf4446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.106113910675049, 3.175016403198242)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Micro overfit test: take a single small batch and train until near-zero loss\n",
    "\n",
    "# Notebook-safe loader settings\n",
    "NUM_WORKERS = 0          # <-- key change (was 2)\n",
    "PREFETCH    = None       # ignored when workers=0\n",
    "PERSISTENT  = False      # must be False when workers=0\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "micro_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=PrebuiltBatchSampler(batches),\n",
    "    collate_fn=collate_pad,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "\n",
    "# Capture initial loss\n",
    "batch = next(iter(micro_loader))\n",
    "for k in batch: batch[k] = batch[k].to(device)\n",
    "with torch.no_grad():\n",
    "    init_loss = model(**batch)[\"loss\"].item()\n",
    "\n",
    "# Train on the same batch repeatedly\n",
    "for _ in range(200):\n",
    "    out = model(**batch)\n",
    "    loss = out[\"loss\"]\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "final_loss = model(**batch)[\"loss\"].item()\n",
    "init_loss, final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687bffc",
   "metadata": {},
   "source": [
    "\n",
    "## 6) (Optional) DistributedSampler preview\n",
    "If you launch with `torchrun`, **DistributedSampler** gives each GPU a different shard:\n",
    "```python\n",
    "sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=False)\n",
    "for epoch in range(num_epochs):\n",
    "    sampler.set_epoch(epoch)  # reshuffle each epoch\n",
    "    loader = DataLoader(dataset, batch_size=BS, sampler=sampler, collate_fn=collate_pad, ...)\n",
    "```\n",
    "For this single-process demo, we just construct it to show API shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db8a335f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m PERSISTENT  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m      \u001b[38;5;66;03m# must be False when workers=0\u001b[39;00m\n\u001b[1;32m      7\u001b[0m PIN_MEMORY  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m----> 9\u001b[0m dl_dist \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPrebuiltBatchSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersistent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPERSISTENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEMORY\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dl_dist))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/torch/utils/data/dataloader.py:355\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# auto_collation with custom batch_sampler\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m shuffle \u001b[38;5;129;01mor\u001b[39;00m sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m drop_last:\n\u001b[0;32m--> 355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_sampler option is mutually exclusive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith batch_size, shuffle, sampler, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     drop_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last"
     ]
    }
   ],
   "source": [
    "\n",
    "# Demo: use DistributedSampler in single-process mode (world_size=1)\n",
    "ds_sampler = DistributedSampler(dataset, num_replicas=1, rank=0, shuffle=True, drop_last=False)\n",
    "# Notebook-safe loader settings\n",
    "NUM_WORKERS = 0          # <-- key change (was 2)\n",
    "PREFETCH    = None       # ignored when workers=0\n",
    "PERSISTENT  = False      # must be False when workers=0\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "dl_dist = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    sampler=ds_sampler,\n",
    "    batch_sampler=PrebuiltBatchSampler(batches),\n",
    "    collate_fn=collate_pad,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "next(iter(dl_dist))[\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01882e8f-44e0-40c8-82dc-d4a59b9e1959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
