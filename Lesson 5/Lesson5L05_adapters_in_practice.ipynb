{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97097e0c",
   "metadata": {},
   "source": [
    "# Adapters in Practice — from Decision to Deployment\n",
    "\n",
    "This notebook follows **Lesson 5** in order: decision ladder → data schema → baseline prompt → LoRA adapters → QLoRA → validation → packaging/rollback → run cards. It is designed to run on a single GPU (12–16 GB VRAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a723471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "# 0) Preamble & installs (Colab/clean env)\n",
    "!pip -q install -U transformers peft datasets accelerate bitsandbytes evaluate rank_bm25\n",
    "\n",
    "import os, re, json, random, math, textwrap\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          TrainingArguments, Trainer, pipeline)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bf555",
   "metadata": {},
   "source": [
    "## 1) Task & data: one schema + fixed dev set\n",
    "We'll create a small synthetic dataset with the schema:\n",
    "**Instruction / Input / Expected Response**. Keep a **fixed dev set** for validation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56dcd954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a tiny synthetic dataset in the class schema\n",
    "raw = [\n",
    "  {\"input\": 'vpn down hr floor—users can’t log in', \"label\": \"Network\", \"title_clean\": \"HR floor VPN outage—login failures\"},\n",
    "  {\"input\": 'reset password portal loop', \"label\": \"Account\", \"title_clean\": \"Password reset portal loop\"},\n",
    "  {\"input\": 'email bounce for vendors', \"label\": \"Email\", \"title_clean\": \"Vendor email bounces\"},\n",
    "  {\"input\": 'wifi flaky conference room b', \"label\": \"Network\", \"title_clean\": \"Conference Room B Wi-Fi unstable\"},\n",
    "  {\"input\": 'new hire cannot access jira', \"label\": \"Access\", \"title_clean\": \"New hire cannot access Jira\"},\n",
    "]\n",
    "\n",
    "def jitter(s):\n",
    "    return s.replace(\"—\",\" - \")\n",
    "\n",
    "data = []\n",
    "for r in raw:\n",
    "    for _ in range(40):  # ~200 examples total\n",
    "        data.append({\n",
    "            \"instruction\": \"Convert this note to a clean ticket title and label.\",\n",
    "            \"input\": jitter(r[\"input\"]),\n",
    "            \"expected\": f'{r[\"title_clean\"]} || Label: {r[\"label\"]}'\n",
    "        })\n",
    "random.shuffle(data)\n",
    "ds = Dataset.from_list(data).train_test_split(test_size=0.15, seed=7)\n",
    "dev = ds[\"test\"]   # fixed dev set\n",
    "train = ds[\"train\"]\n",
    "len(train), len(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754026b1",
   "metadata": {},
   "source": [
    "### Simple validation helpers\n",
    "**Format-pass** (does the output follow `<title> || Label: <LABEL>`?) and a rough **label-accuracy proxy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3dd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pred(pred:str):\n",
    "    m = re.match(r'(.+?)\\s*\\|\\|\\s*Label:\\s*([A-Za-z]+)', pred.strip())\n",
    "    return {\"format_ok\": bool(m), \"label\": (m.group(2) if m else None)}\n",
    "\n",
    "def evaluate_preds(preds, refs):\n",
    "    ok = 0; label_ok = 0\n",
    "    for p, r in zip(preds, refs):\n",
    "        pp = parse_pred(p)\n",
    "        ok += pp[\"format_ok\"]\n",
    "        label_ok += int(pp[\"label\"] in r[\"expected\"])  # proxy: label string present\n",
    "    n = len(preds)\n",
    "    return {\"format_pass\": ok/n, \"label_acc_proxy\": label_ok/n}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3ec30",
   "metadata": {},
   "source": [
    "## 2) Baseline: Prompt-only (optionally with tiny retrieval)\n",
    "Start with the lightest approach and measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d31a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d0432b1ad14441a17db6d5f69c66db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9958c8574294bf0962e3b098c696fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7593fdd288440108f847c47109b4892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/452k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288660be3b96487a850be0372631c7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model sshleifer/tiny-gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5097, in from_pretrained\n    config, dtype, dtype_orig = _get_dtype(\n                                ^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1358, in _get_dtype\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5179, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5445, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 838, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'device_map')\n\nwhile loading with GPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5097, in from_pretrained\n    config, dtype, dtype_orig = _get_dtype(\n                                ^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1358, in _get_dtype\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5179, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5445, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 838, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'device_map')\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msshleifer/tiny-gpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msshleifer/tiny-gpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m prompt_tmpl \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruction: Convert this note to a clean ticket title and label.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{inp}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespond EXACTLY like: <title> || Label: <OneWordLabel>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_baseline\u001b[39m(batch):\n",
      "File \u001b[0;32m~/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1028\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1027\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m-> 1028\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py:333\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    332\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model sshleifer/tiny-gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5097, in from_pretrained\n    config, dtype, dtype_orig = _get_dtype(\n                                ^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1358, in _get_dtype\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5179, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5445, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 838, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'device_map')\n\nwhile loading with GPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5097, in from_pretrained\n    config, dtype, dtype_orig = _get_dtype(\n                                ^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1358, in _get_dtype\n    state_dict = load_state_dict(\n                 ^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5179, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5445, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 532, in load_state_dict\n    check_torch_load_is_safe()\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1632, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 2929, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 838, in __init__\n    super().__init__(config, *inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 1190, in __init__\n    super().__init__(*inputs, **kwargs)\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/jerhenry/Documents/Workdoc/f-Virtual_Machines/Environments/Pytorch_nov2024/lib/python3.11/site-packages/tf_keras/src/utils/generic_utils.py\", line 513, in validate_kwargs\n    raise TypeError(error_message, kwarg)\nTypeError: ('Keyword argument not understood:', 'device_map')\n\n\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\", tokenizer=\"sshleifer/tiny-gpt2\", device_map=\"auto\")\n",
    "\n",
    "prompt_tmpl = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"Instruction: Convert this note to a clean ticket title and label.\\n\"\n",
    "    \"Input: {inp}\\n\"\n",
    "    \"Respond EXACTLY like: <title> || Label: <OneWordLabel>\\n\"\n",
    ")\n",
    "\n",
    "def gen_baseline(batch):\n",
    "    outs = []\n",
    "    for x in batch[\"input\"]:\n",
    "        text = gen(prompt_tmpl.format(inp=x), max_new_tokens=32, do_sample=False)[0][\"generated_text\"]\n",
    "        outs.append(text.splitlines()[-1])  # take last line\n",
    "    return outs\n",
    "\n",
    "baseline_preds = gen_baseline(dev)\n",
    "evaluate_preds(baseline_preds, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9665285",
   "metadata": {},
   "source": [
    "## 3) LoRA adapters: Q & V on a few layers (beginner defaults)\n",
    "Train a **tiny** LoRA head to stabilize format and labels. **LR = learning rate** here; **rank (r)** is the LoRA size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"sshleifer/tiny-gpt2\"  # tiny for demo; swap for a small chat model on better hardware\n",
    "tok = AutoTokenizer.from_pretrained(BASE)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, device_map=\"auto\")\n",
    "\n",
    "def to_text(example):\n",
    "    return f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nExpected Response:\"\n",
    "\n",
    "train_texts = [to_text(x) for x in train]\n",
    "dev_texts = [to_text(x) for x in dev]\n",
    "labels = [x[\"expected\"] for x in train]\n",
    "labels_dev = [x[\"expected\"] for x in dev]\n",
    "\n",
    "def format_for_causal_lm(prompts, targets, tok, max_len=256):\n",
    "    enc = tok(prompts, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    with tok.as_target_tokenizer():\n",
    "        lab = tok(targets, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_len)\n",
    "    X, Y = [], []\n",
    "    for p, t in zip(enc[\"input_ids\"], lab[\"input_ids\"]):\n",
    "        x = torch.cat([p, t], dim=0)[:max_len]\n",
    "        y = torch.cat([torch.full_like(p, -100), t], dim=0)[:max_len]\n",
    "        X.append(x); Y.append(y)\n",
    "    return Dataset.from_dict({\"input_ids\": X, \"labels\": Y})\n",
    "\n",
    "train_ds = format_for_causal_lm(train_texts, labels, tok)\n",
    "dev_ds   = format_for_causal_lm(dev_texts, labels_dev, tok)\n",
    "\n",
    "# LoRA config — tiny-gpt2 uses c_attn (qkv together). For LLaMA-like models use [\"q_proj\",\"v_proj\"].\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\"],  # Q/V (and K) projection in this tiny model\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_cfg)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6107b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with gradient clipping and simple evaluation schedule\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"out_lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=1e-4,   # LR = learning rate\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    import torch\n",
    "    return {k: torch.nn.utils.rnn.pad_sequence([f[k] for f in features], batch_first=True, padding_value=tok.pad_token_id)\n",
    "            for k in [\"input_ids\",\"labels\"]}\n",
    "\n",
    "trainer = Trainer(model=lora_model, args=args, train_dataset=train_ds, eval_dataset=dev_ds, data_collator=data_collator)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15ea4e",
   "metadata": {},
   "source": [
    "### Quick validation (format-pass & label proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09816cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_with_adapter(model, prompts):\n",
    "    outs = []\n",
    "    for p in prompts:\n",
    "        ids = tok(p, return_tensors=\"pt\").to(model.device)\n",
    "        g = model.generate(**ids, max_new_tokens=32, do_sample=False)\n",
    "        text = tok.decode(g[0], skip_special_tokens=True)\n",
    "        outs.append(text.split(\"Expected Response:\")[-1].strip().splitlines()[0])\n",
    "    return outs\n",
    "\n",
    "preds = gen_with_adapter(trainer.model, dev_texts)\n",
    "evaluate_preds(preds, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66843c3e",
   "metadata": {},
   "source": [
    "## 4) QLoRA: fit in smaller VRAM (4-bit base + FP16/BF16 adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "q_model = AutoModelForCausalLM.from_pretrained(BASE, quantization_config=bnb, device_map=\"auto\")\n",
    "q_model = prepare_model_for_kbit_training(q_model)\n",
    "\n",
    "q_lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"c_attn\"], bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "q_lora = get_peft_model(q_model, q_lora_cfg)\n",
    "# (Optional) Repeat a very small training loop like above if time permits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af846e12",
   "metadata": {},
   "source": [
    "## 5) Knob ablation: rank (r)=8 vs 16 — change one thing at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one(r):\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE, device_map=\"auto\")\n",
    "    cfg = LoraConfig(r=r, lora_alpha=16, lora_dropout=0.05, target_modules=[\"c_attn\"], bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    m = get_peft_model(model, cfg)\n",
    "    tr = Trainer(model=m, args=args, train_dataset=train_ds, eval_dataset=dev_ds, data_collator=data_collator)\n",
    "    tr.train()\n",
    "    preds = gen_with_adapter(tr.model, dev_texts)\n",
    "    return r, evaluate_preds(preds, dev)\n",
    "\n",
    "for r in [8, 16]:\n",
    "    print(run_one(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422df8e3",
   "metadata": {},
   "source": [
    "## 6) Packaging & rollback: save only the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter and tokenizer\n",
    "trainer.model.save_pretrained(\"adapter_v1\")\n",
    "tok.save_pretrained(\"adapter_v1\")\n",
    "\n",
    "# Load in a fresh session and apply the adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE, device_map=\"auto\")\n",
    "adapted = PeftModel.from_pretrained(base, \"adapter_v1\")\n",
    "# Rollback = just use `base` without loading the adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8adbb",
   "metadata": {},
   "source": [
    "## 7) Run Card (log your experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f450e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_card = {\n",
    "  \"base\": BASE,\n",
    "  \"adapter\": {\"r\": 8, \"alpha\": 16, \"targets\": [\"c_attn (Q/V)\" ]},\n",
    "  \"learning_rate\": 1e-4,  # LR = learning rate\n",
    "  \"dev_metrics\": evaluate_preds(preds, dev),\n",
    "  \"decision\": \"keep / try higher rank\",\n",
    "  \"notes\": \"schema clean, masks ok\"\n",
    "}\n",
    "with open(\"run_card.json\",\"w\") as f:\n",
    "    json.dump(run_card, f, indent=2)\n",
    "run_card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95b49f",
   "metadata": {},
   "source": [
    "## 8) Optional: Pick‑a‑Path presets (for classroom toggles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    " \"ticket_tidy\": {\"strategy\":\"adapters\", \"r\":8, \"targets\":[\"qv\"], \"metric\":\"format_pass\"},\n",
    " \"policy_qa\":   {\"strategy\":\"prompt_rag\", \"few_shot\":3, \"citation\":True},\n",
    " \"brand_style\": {\"strategy\":\"adapters\", \"r\":16, \"targets\":[\"qv\",\"o\"], \"metric\":\"style_match\"}\n",
    "}\n",
    "print(json.dumps(paths, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
